{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, random, gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "EPISODE = 1000\n",
    "EPOCHS = 4\n",
    "ITERS = 64\n",
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "EPSILON = 0.1\n",
    "EPS = 1e-8\n",
    "TEST_EPISODE = 50\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    def __init__(self, maxlen, state_shape):\n",
    "        super(PPOBuffer, ).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.idx = 0\n",
    "\n",
    "        self.states = np.zeros((maxlen, state_shape))\n",
    "        self.actions = np.zeros(maxlen, dtype=np.int32)\n",
    "        self.log_probs = np.zeros(maxlen, dtype=np.float32)\n",
    "        self.critic_values = np.zeros(maxlen+1, dtype=np.float32)\n",
    "        self.rewards = np.zeros(maxlen)\n",
    "        self.mask_done_ep = np.zeros(maxlen)\n",
    "        \n",
    "    def store(self, state, action, log_prob, critic_value, reward, done):\n",
    "        self.states[self.idx] = state\n",
    "        self.actions[self.idx] = action\n",
    "        self.log_probs[self.idx] = log_prob\n",
    "        self.critic_values[self.idx] = critic_value\n",
    "        self.rewards[self.idx] = reward\n",
    "        self.mask_done_ep[self.idx] = 1 - int(done)\n",
    "        self.idx += 1\n",
    "\n",
    "    def reset_buffer(self):\n",
    "        self.idx = 0\n",
    "        \n",
    "    def sample(self, idxs):\n",
    "        states_batch = self.states[idxs]\n",
    "        # this structure is used to gather the action probabilites below\n",
    "        actions_batch = [[i,  self.actions[idx]] for i,idx in enumerate(idxs)] \n",
    "        log_probs_batch =  self.log_probs[idxs]\n",
    "        \n",
    "        return states_batch, actions_batch, log_probs_batch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, \n",
    "                 state_shape,\n",
    "                 n_actions,\n",
    "                 hidden_units,\n",
    "                 optimizer,\n",
    "                 epsilon, \n",
    "                 gamma,\n",
    "                 llambda\n",
    "    ):\n",
    "        \n",
    "        # ppo parameters\n",
    "        self.clip_max = 1 + epsilon\n",
    "        self.clip_min = 1 - epsilon\n",
    "        self.c1 = 1\n",
    "        self.c2 = 0.01\n",
    "        self.gamma = gamma\n",
    "        self.llambda = llambda\n",
    "        \n",
    "        # policy-critic network\n",
    "        self.n_actions = n_actions\n",
    "        self.actor_critic = self.get_actor_critic_model(state_shape, n_actions, hidden_units)\n",
    "        self.optim = optimizer\n",
    "        \n",
    "    def get_actor_critic_model(self, state_shape, n_actions, hidden_units):\n",
    "        inputs = layers.Input(shape=(state_shape,))\n",
    "        x = layers.Dense(hidden_units[0], activation='relu')(inputs)\n",
    "        x = layers.Dense(hidden_units[1], activation='relu')(x)\n",
    "        actor = layers.Dense(n_actions, activation='softmax', name='actor')(x)\n",
    "        critic = layers.Dense(1, activation='linear', name='critic')(x)\n",
    "\n",
    "        return keras.Model(inputs=inputs, outputs=[actor,critic])\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        actions_prob, critic_val = self.actor_critic(state)\n",
    "        action = np.random.choice(self.n_actions, p=np.squeeze(actions_prob.numpy()))\n",
    "        \n",
    "        return action, tf.squeeze(actions_prob), tf.math.log(tf.squeeze(actions_prob)), tf.squeeze(critic_val)\n",
    "    \n",
    "    def compute_advantages(self, rewards, critic_values, masks_done_ep, last_state):\n",
    "        critic_values[-1] = tf.squeeze(self.actor_critic(tf.expand_dims(last_state, 0))[1]).numpy()\n",
    "        deltas = rewards + self.gamma * critic_values[1:] * masks_done_ep - critic_values[:-1]\n",
    "        a_t = 0\n",
    "        advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            a_t = deltas[t] + self.gamma * self.llambda * masks_done_ep[t] * a_t\n",
    "            advantages[t] = a_t\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, states, actions, log_probabilities, advantages, returns):\n",
    "        with tf.GradientTape() as tape:\n",
    "            probs, values = self.actor_critic(states)\n",
    "            log_probs = tf.math.log(tf.gather_nd(probs, actions))\n",
    "            \n",
    "            # actor loss\n",
    "            ratio = tf.math.exp(log_probs - log_probabilities)\n",
    "            l_clip = - tf.math.minimum(\n",
    "                    ratio * advantages,                                                  # normal ratio\n",
    "                    tf.clip_by_value(ratio, self.clip_min, self.clip_max) * advantages,  # clip term\n",
    "                )\n",
    "\n",
    "            # critic loss\n",
    "            l_vf = keras.losses.huber(returns, values)\n",
    "            \n",
    "            # entropy term\n",
    "            s = - tf.reduce_sum(probs * tf.math.log(probs + 1e-8), axis=-1)\n",
    "            \n",
    "            loss = tf.reduce_mean (\n",
    "                l_clip + self.c1 * l_vf - self.c2 * s\n",
    "            )\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.actor_critic.trainable_variables)\n",
    "        self.optim.apply_gradients(zip(gradients, self.actor_critic.trainable_variables))\n",
    "        \n",
    "    def test(self, env):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _, _, _ = self.select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += 1\n",
    "        return total_reward\n",
    "        \n",
    "    def save_weight(self, name):\n",
    "        self.actor_critic.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_shape = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "ppo_buffer = PPOBuffer(maxlen=ITERS, state_shape=state_shape)\n",
    "agent = PPO(state_shape=state_shape,\n",
    "          n_actions=n_actions,\n",
    "          hidden_units=[32,16],\n",
    "          optimizer=optimizer,\n",
    "          epsilon=EPSILON, \n",
    "          gamma=GAMMA,\n",
    "          llambda=LAMBDA\n",
    ")\n",
    "\n",
    "for episode in range(1, EPISODE+1):\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    for t in range(ITERS):\n",
    "        action, actions_prob, actions_log_prob, critic_val = agent.select_action(state=state)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        ppo_buffer.store(state, action, actions_log_prob[action].numpy(), critic_val.numpy(), reward, done)\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "            continue\n",
    "        \n",
    "        state = next_state\n",
    "            \n",
    "    # compute advantages using gae\n",
    "    advantages = agent.compute_advantages(ppo_buffer.rewards, ppo_buffer.critic_values, ppo_buffer.mask_done_ep, next_state)\n",
    "    discrounted_returns = advantages + ppo_buffer.critic_values[:-1]\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    batch_idx_size = (int(ITERS/BATCH_SIZE), BATCH_SIZE)\n",
    "    for epoch in range(EPOCHS):\n",
    "        idxs = np.random.choice(range(ITERS), size=batch_idx_size, replace=False)\n",
    "        for batch_idxs in idxs:\n",
    "            states_batch, actions_batch, log_probs_batch = ppo_buffer.sample(batch_idxs)\n",
    "            advantages_batch = advantages[batch_idxs]\n",
    "            returns_batch = discrounted_returns[batch_idxs]\n",
    "  \n",
    "            agent.train(states_batch, actions_batch, log_probs_batch, advantages_batch, returns_batch)\n",
    "\n",
    "    ppo_buffer.reset_buffer()\n",
    "    \n",
    "    if running_rewards[-1] > TEST_EPISODE:\n",
    "        print(f'Solved at {episode} espisode: running_reward={running_rewards[-1]:.2f}')\n",
    "        agent.save_weight(f\"checkpoints/ppo_lunarlander_final.h5\")\n",
    "        break\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
